# üîç SEO - Sitemap et Robots.txt

> G√©n√©ration automatique des fichiers SEO pour Google Search Console

## üìã Vue d'ensemble

Ogan Framework fournit des outils pour g√©n√©rer automatiquement :
- **`sitemap.xml`** : Liste des URLs indexables par les moteurs de recherche
- **`robots.txt`** : Instructions pour les crawlers (ce qu'ils peuvent/ne peuvent pas visiter)

---

## üöÄ Commandes Console

### G√©n√©rer tous les fichiers SEO

```bash
php bin/console seo:all --base-url=https://votre-site.com
```

### G√©n√©rer sitemap uniquement

```bash
php bin/console seo:sitemap --base-url=https://votre-site.com
```

### G√©n√©rer robots.txt uniquement

```bash
php bin/console seo:robots --base-url=https://votre-site.com
```

### Options disponibles

| Option | Description | D√©faut |
|--------|-------------|--------|
| `--base-url` | URL de base du site | `https://example.com` |
| `--output` | Dossier de sortie | `public/` |

---

## üó∫Ô∏è SitemapGenerator

### Utilisation basique

```php
use Ogan\Seo\SitemapGenerator;

$sitemap = new SitemapGenerator('https://votre-site.com');

// Ajouter des URLs manuellement
$sitemap->addUrl('/', priority: 1.0)
        ->addUrl('/about', priority: 0.8)
        ->addUrl('/contact', priority: 0.6)
        ->addUrl('/blog', changefreq: 'daily');

$sitemap->save('public/sitemap.xml');
```

### G√©n√©ration automatique depuis les routes

```php
use Ogan\Seo\SitemapGenerator;
use Ogan\Router\Router;

$router = new Router();
$router->loadRoutesFromControllers(__DIR__ . '/src/Controller');

$sitemap = new SitemapGenerator('https://votre-site.com');
$sitemap->addRoutesFromRouter($router);
$sitemap->save('public/sitemap.xml');
```

### Param√®tres de `addUrl()`

| Param√®tre | Type | Description | D√©faut |
|-----------|------|-------------|--------|
| `$path` | string | Chemin de l'URL | - |
| `$lastmod` | string\|null | Date de modification (ISO 8601) | Date du jour |
| `$changefreq` | string | Fr√©quence de mise √† jour | `weekly` |
| `$priority` | float | Priorit√© (0.0 √† 1.0) | `0.5` |

**Valeurs de `changefreq` :** `always`, `hourly`, `daily`, `weekly`, `monthly`, `yearly`, `never`

### Patterns d'exclusion

Par d√©faut, les routes suivantes sont exclues :
- `/admin*`
- `/api*`
- `/login`, `/logout`, `/register`
- `/forgot-password`, `/reset-password*`

Personnaliser les exclusions :

```php
$sitemap->setExcludePatterns([
    '/admin*',
    '/private*',
]);

// Ou ajouter un pattern
$sitemap->addExcludePattern('/members-only*');
```

### R√©sultat g√©n√©r√©

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://votre-site.com/</loc>
    <lastmod>2025-12-24</lastmod>
    <changefreq>weekly</changefreq>
    <priority>1.0</priority>
  </url>
</urlset>
```

---

## ü§ñ RobotsGenerator

### Utilisation basique

```php
use Ogan\Seo\RobotsGenerator;

$robots = new RobotsGenerator('https://votre-site.com');
// Les r√®gles par d√©faut sont appliqu√©es automatiquement

$robots->sitemap('/sitemap.xml')
       ->save('public/robots.txt');
```

### R√®gles par d√©faut

Le g√©n√©rateur bloque automatiquement :
- `/admin/`
- `/api/`
- `/login`, `/logout`, `/register`
- `/forgot-password`, `/reset-password`, `/verify-email`

### Personnaliser les r√®gles

```php
$robots = new RobotsGenerator('https://votre-site.com', withDefaults: false);

$robots->allow('/')
       ->disallow('/admin/')
       ->disallow('/private/')
       ->sitemap('/sitemap.xml')
       ->crawlDelay(2)  // D√©lai entre les requ√™tes
       ->save();
```

### R√®gles par user-agent

```php
$robots = new RobotsGenerator('https://votre-site.com');

// R√®gles pour tous les bots
$robots->allow('/')
       ->disallow('/admin/');

// R√®gles sp√©cifiques pour Googlebot
$robots->userAgent('Googlebot')
       ->allow('/special-page/')
       ->disallow('/no-google/');

$robots->save();
```

### R√©sultat g√©n√©r√©

```
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/
Disallow: /login
Disallow: /logout

Sitemap: https://votre-site.com/sitemap.xml
```

---

## üì§ Soumettre √† Google Search Console

1. **G√©n√©rer les fichiers :**
   ```bash
   php bin/console seo:all --base-url=https://votre-site.com
   ```

2. **V√©rifier les fichiers :**
   - `https://votre-site.com/sitemap.xml`
   - `https://votre-site.com/robots.txt`

3. **Soumettre le sitemap :**
   - Aller sur [Google Search Console](https://search.google.com/search-console)
   - S√©lectionner votre propri√©t√©
   - Aller dans "Sitemaps" ‚Üí Ajouter `sitemap.xml`

4. **Tester robots.txt :**
   - Utiliser l'outil [Robots Testing Tool](https://www.google.com/webmasters/tools/robots-testing-tool)

---

## üîÑ Automatisation

### Via cron (r√©g√©n√©ration quotidienne)

```bash
# crontab -e
0 2 * * * cd /var/www/mysite && php bin/console seo:all --base-url=https://votre-site.com
```

### Via √©v√©nement de d√©ploiement

Ajoutez dans votre script de d√©ploiement :

```bash
composer install --no-dev
php bin/console cache:clear
php bin/console seo:all --base-url=https://votre-site.com
```

---

## ‚úÖ Bonnes Pratiques

1. **R√©g√©n√©rer apr√®s chaque d√©ploiement** pour inclure les nouvelles pages
2. **Utiliser des priorit√©s coh√©rentes** : page d'accueil (1.0), pages importantes (0.8), autres (0.5)
3. **Ne pas indexer les pages admin/API** - d√©j√† exclu par d√©faut
4. **Garder le sitemap √† jour** avec les bonnes dates de modification
5. **Tester r√©guli√®rement** l'accessibilit√© du sitemap via Search Console
